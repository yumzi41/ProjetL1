{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yumzi41/ProjetL1/blob/master/exercices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/yumzi41/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd738e0d-10c7-4286-9431-42e552716128"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exam_2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 200"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 1** *Une relation linéaire* \\\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef `'inputs'` donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef `'targets'` renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle :  \n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon $$  \n",
        "où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$. \\\n",
        "\n",
        "Nous allons estimer les paramètres $\\theta_0$, $\\theta_1$, $\\theta_2$ et $\\theta_3$ à l’aide de la méthode analytique des moindres carrés, et évaluer l'erreur quadratique moyenne (*Mean Squared Error*, MSE) sur les jeux d'entraînement et de test.\n"
      ],
      "metadata": {
        "id": "zNULWtN1YE0l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKnxnpPemHUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Génération des jeux de données\n",
        "train_set, test_set = generate_dataset(mySeed=42)\n",
        "X_train, t_train = train_set['inputs'], train_set['targets']\n",
        "X_test, t_test = test_set['inputs'], test_set['targets']\n",
        "\n",
        "# Ajout d'une colonne de 1 pour le biais θ0\n",
        "X_train_aug = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
        "X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
        "\n",
        "# Estimation des paramètres : θ = (X^T X)^(-1) X^T t\n",
        "theta = np.linalg.inv(X_train_aug.T @ X_train_aug) @ (X_train_aug.T @ t_train)\n",
        "\n",
        "# Prédictions et calcul des erreurs quadratiques moyennes\n",
        "t_train_pred = X_train_aug @ theta\n",
        "t_test_pred = X_test_aug @ theta\n",
        "mse_train = np.mean((t_train_pred - t_train) ** 2)\n",
        "mse_test = np.mean((t_test_pred - t_test) ** 2)\n",
        "\n",
        "print(\"Paramètres estimés (θ0, θ1, θ2, θ3) :\", theta)\n",
        "print(f\"MSE sur le train : {mse_train:.3f}\")\n",
        "print(f\"MSE sur le test  : {mse_test:.3f}\")\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2ee9c1-d7ab-45cf-9cb4-ff6262ca3d98"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paramètres estimés (θ0, θ1, θ2, θ3) : [2.1316691  0.46735151 0.49115291 0.73864314]\n",
            "MSE sur le train : 4.285\n",
            "MSE sur le test  : 3.935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2**  \n",
        "Pour estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD, l’architecture appropriée dépend de la nature des données et de la relation entre les prédicteurs $(x, y, z)$ et la cible $t$.  \n",
        "\n",
        "### Choix d'architecture  \n",
        "- Une **couche linéaire simple** (sans couche cachée) est suffisante si la relation entre les prédicteurs et la cible est strictement linéaire :  \n",
        "  \\[\n",
        "  t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon\n",
        "  \\]\n",
        "  Cette architecture permet d’estimer directement les coefficients $\\theta_k$ et offre une **interprétabilité totale**.  \n",
        "\n",
        "- Une **architecture avec une couche cachée** (par exemple, $3 \\rightarrow 16 \\rightarrow 1$ avec activation ReLU) peut être utilisée si des **non-linéarités** sont présentes dans les données. Cela améliore la **flexibilité** et permet de capturer des relations plus complexes, mais au prix d’une **perte d’interprétabilité**.  \n",
        "\n",
        "### Justification  \n",
        "- **Expressivité** : Une couche cachée avec des activations non linéaires permet de modéliser des relations non linéaires. Cependant, pour une tâche de régression linéaire stricte, une seule couche linéaire suffit.  \n",
        "- **Performances en généralisation** : Une architecture plus complexe risque de surapprendre, surtout avec un petit dataset. Une simple couche linéaire favorise une meilleure généralisation si la relation est purement linéaire.\n",
        "\n",
        "### Conclusion  \n",
        "Pour ce problème, où les données suivent une relation linéaire définie par $\\theta_k$, une **architecture avec une seule couche linéaire** est idéale. Voici le code correspondant.\n"
      ],
      "metadata": {
        "id": "gKM7AUNzZm8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        # Réseau linéaire simple : 3 entrées (x, y, z), 1 sortie (t)\n",
        "        self.fc = nn.Linear(3, 1)  # Pas de couche cachée, directement une sortie\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On détecte si un GPU est disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        # Passage sur GPU\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        # Remise à zéro des gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = mySimpleNet(batch_inputs)\n",
        "\n",
        "        # Calcul de la loss\n",
        "        loss = criterion(outputs.squeeze(), batch_targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Mise à jour des paramètres\n",
        "        optimizer.step()\n",
        "\n",
        "    # Affichage occasionnel de la loss\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a8b33e-3eba-42b9-9eaa-6b354be11f42"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 5.3792\n",
            "Epoch [100/500], Loss: 4.4078\n",
            "Epoch [150/500], Loss: 5.3685\n",
            "Epoch [200/500], Loss: 3.8084\n",
            "Epoch [250/500], Loss: 3.2484\n",
            "Epoch [300/500], Loss: 3.7801\n",
            "Epoch [350/500], Loss: 3.9642\n",
            "Epoch [400/500], Loss: 4.7946\n",
            "Epoch [450/500], Loss: 4.9167\n",
            "Epoch [500/500], Loss: 3.9054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OY-HmbNGa-ZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4**  \n",
        "Pour un **réseau purement linéaire** (une seule couche `nn.Linear(3, 1)`), les estimations des $\\theta_k$ se trouvent dans :  \n",
        "- Le **biais** de la couche linéaire pour $\\theta_0$,  \n",
        "- Les **poids** de la couche linéaire pour $\\theta_1, \\theta_2, \\theta_3$.  \n",
        "\n",
        "Dans le cas d’un réseau à **couche(s) cachée(s)**, il n’y a plus d’équivalence directe avec les $\\theta_k$ d’une simple régression linéaire, car les poids sont combinés avec une **activation non linéaire**. Pour retrouver un modèle linéaire pur, on doit retirer la (les) couche(s) cachée(s).\n",
        "\n",
        "Voici un exemple de code qui utilise une **seule couche linéaire** et montre l’extraction de $\\theta_0, \\theta_1, \\theta_2, \\theta_3$ :\n"
      ],
      "metadata": {
        "id": "kn7LLN2ua96x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction des poids (θ1, θ2, θ3) et du biais (θ0)\n",
        "theta_nn = mySimpleNet.fc.weight.data.cpu().numpy().squeeze()  # shape = (3,)\n",
        "theta0 = mySimpleNet.fc.bias.data.cpu().numpy().item()      # scalaire\n",
        "\n",
        "print(f\"θ0 (biais)   = {theta0:.4f}\")\n",
        "print(f\"θ1 (poids x) = {theta_nn[0]:.4f}\")\n",
        "print(f\"θ2 (poids y) = {theta_nn[1]:.4f}\")\n",
        "print(f\"θ3 (poids z) = {theta_nn[2]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21255d4-0cb9-4838-d43b-370d3d0d3312"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "θ0 (biais)   = 2.0639\n",
            "θ1 (poids x) = 0.4634\n",
            "θ2 (poids y) = 0.4910\n",
            "θ3 (poids z) = 0.8507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des paramètres estimés avec la méthode des moindres carrés\n",
        "print(\"Paramètres estimés avec la méthode des moindres carrés :\")\n",
        "print(f\"θ0 : {theta[0]:.4f}\")\n",
        "print(f\"θ1 : {theta[1]:.4f}\")\n",
        "print(f\"θ2 : {theta[2]:.4f}\")\n",
        "print(f\"θ3 : {theta[3]:.4f}\")\n",
        "\n",
        "print(\"\\nParamètres estimés avec le réseau de neurones :\")\n",
        "print(f\"θ0 : {theta0:.4f}\")\n",
        "print(f\"θ1 : {theta_nn[0]:.4f}\")\n",
        "print(f\"θ2 : {theta_nn[1]:.4f}\")\n",
        "print(f\"θ3 : {theta_nn[2]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNFEkbSVfZLR",
        "outputId": "07a9bf59-c8d4-491b-abdb-b498f5fde5d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paramètres estimés avec la méthode des moindres carrés :\n",
            "θ0 : 2.1317\n",
            "θ1 : 0.4674\n",
            "θ2 : 0.4912\n",
            "θ3 : 0.7386\n",
            "\n",
            "Paramètres estimés avec le réseau de neurones :\n",
            "θ0 : 2.0639\n",
            "θ1 : 0.4634\n",
            "θ2 : 0.4910\n",
            "θ3 : 0.8507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Commentaires :\n",
        "- Les valeurs des \\(\\theta_k\\) estimées par la méthode des **moindres carrés** sont très proches de celles obtenues avec le **réseau de neurones**.\n",
        "- La légère différence entre les deux estimations est due au fait que la méthode des moindres carrés fournit une solution exacte pour un modèle linéaire, tandis que le réseau de neurones utilise une optimisation approchée via SGD.\n",
        "- Cette comparaison montre que le réseau de neurones a bien appris la relation linéaire présente dans les données.\n"
      ],
      "metadata": {
        "id": "HGu1xXoJfyui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a4af20-d515-4f1d-fb53-2d50866ccebc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "\n",
        "import torch\n",
        "\n",
        "# On réutilise la classe causalFCN telle que définie dans le code fourni\n",
        "model = causalFCN()\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Nombre total de paramètres du réseau : {num_params}\")\n"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9881b2e-023d-4b9c-cfd3-bef3dc55afa0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres du réseau : 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?\n",
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*.\n",
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*.\n",
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)\n",
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant. , pour toutes ces questions  "
      ],
      "metadata": {
        "id": "9jICtf3GizQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}